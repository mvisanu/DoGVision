# -*- coding: utf-8 -*-
"""dog-vision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nXRYPbn0xBoTqCTarOW0Ly02THe0M8vv

# üê∂ End-to-end Multi-Class Dog Breed Classification

This notebook builds an end-to-end multi-class image classifier using TensorFlow 2.0 and TensorFlow Hub.

## 1. Problem

Identifying a breed of a dog given an image of a dog.

When I'm sitting at the cafe and I take a photo of a dog, I want to know what breed of dog it is.

## 2. Data

The data we are using is the Kaggle's dog breed identification competition.

https://www.kaggle.com/c/dog-breed-identification/data

## 3. Evaluation

The evaluation is a file with prediction probabilities for each dog breed of each test image.

https://www.kaggle.com/c/dog-breed-identification/overview/evaluation

## 4. Features

Some information about the data:
* We're dealing with images (unstructured data) so it's probably best we use deep learning/transfer learning.
* There are 120 breeds of dogs (this mean there are 120 different classes)
* There are around 10,000+ images in the training set
(these images have breed labels)
* There are around 10,000+ images in the test set
(these images have no breed label, because we'll want to predict the dog breed)
"""

# unzip data from kaggle in google drive
#!unzip "drive/MyDrive/Dog Vision/dog-breed-identification.zip" -d "drive/MyDrive/Dog Vision/"

"""## Getting our workspace ready

* import tensorflow 2.x ‚úÖ
* Import TensorFlow Hub
* Make sure we're using a GPU
"""

# Import TensorFlow and TensorFlow Hub into Colab
import tensorflow as tf
import tensorflow_hub as hub

print("TF version:", tf.__version__)
print("TF Hub version:", hub.__version__)

# Check for GPU availability
print("GPU", "avaiable (YESSSS!!!!!! :)" if tf.config.list_logical_devices("GPU") else "not available :(")

# Import TF 2.x
#try:
  # %tensorflow_version only exists in Colab
#  %tensorflow_version 2.X
#except Exception:
#  pass

"""## Getting our data ready (turning our data into Tensors)

With all machine learning models, our data has to be in numerical format. So that's what we'll be doing first.  Turning our images into tensors numerical representations.

Let's start by accessing our data and checking out the labels.
"""

# Checkout the labels of our data
import pandas as pd
labels_csv = pd.read_csv("drive/MyDrive/Dog Vision/labels.csv")
print(labels_csv.describe())
print(labels_csv.head())

labels_csv.drop('Unnamed: 3', axis=1, inplace=True)

labels_csv.drop('Unnamed: 2', axis=1, inplace=True)

labels_csv.head()

#labels_csv.columns

# How many images are there of each breed?
labels_csv["breed"].value_counts().plot.bar(figsize=(20, 10))

labels_csv["breed"].value_counts().median()

# Let's view an image
from IPython.display import Image
Image("drive/MyDrive/Dog Vision/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg")

"""### Getting images and their labels

Let's get a list of all of our images file and path names
"""

labels_csv.head()

imagePath = "drive/MyDrive/Dog Vision/train/"

# Create pathnames from image ID's
filenames = [imagePath + fname + ".jpg" for fname in labels_csv["id"]]

# Check the first 10
filenames[:10]

# Check whether number of filenames matches number of actual image files
import os
if len(os.listdir(imagePath)) == len(filenames):
  print("Filenames match actual amount of files!!! proceed")
else:
  print("Filenames do not match check image directory")

# One more check
Image(filenames[9000])

labels_csv["breed"][9000]

"""Since we've now got our training image filepaths in a list, let's prepare our labels."""

import numpy as np

labels = labels_csv['breed'].to_numpy()
#labels = np.array(labels)  #does the same thing as to_numpy()
labels

len(labels)

# See if number of labels matches the number of filenames
if len(labels) == len(filenames):
  print("Number of labels matches number of filenames!")
else:
  print("Number of labels does not match number of filenames, check data directories")

# Find the unique label values

unique_breeds = np.unique(labels)
len(unique_breeds)

# Turn a single label into an array of booleans
print(labels[0])
labels[0] == unique_breeds

# Turn every label into a boolean array
boolean_labels = [label == unique_breeds for label in labels]
boolean_labels[:2]

len(boolean_labels)

len(labels)

# Example: Turning boolean array into integers
print(labels[0]) # original label
print(np.where(unique_breeds == labels[0])) # index where label occurs
print(boolean_labels[0].argmax())  # index where label occurs in boolean array
print(boolean_labels[0].astype(int)) # there will be a 1 where the sample label occurs

print(labels[2])
print(boolean_labels[2].astype(int))

filenames[:10]

boolean_labels[:2]

"""### Creating our own validation set

Since kaggle does not give us a validation set we are going to create our own
"""

# Setup X & y variables
X = filenames
y = boolean_labels

"""We're going to start off experimenting with ~1000 images and increases as needed"""

# Set number of images to use for experimenting
NUM_IMAGES = 1000 #@param{type:"slider", min:1000, max:10000, step:10}

# Let's split our data into train and validation sets
from sklearn.model_selection import train_test_split

# Split them into training and validation of total size NUM_IMAGES
X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],
                                                  y[:NUM_IMAGES],
                                                  test_size=0.2,
                                                  random_state=42)

len(X_train), len(y_train), len(X_val), len(y_val)

# Let's have a geez at the training data
X_train[:5], y_train[:2]

"""### Preprocessing Images (turning images into Tensors (numerical values))

To preprocess our images into Tensors we're going to write a function which does the following:
1. Take an image filepath as input
2. Use TensorFlow to read the file and save it to a variable, `image`
3. Turn our `image` (a jpg) into Tensors
4. Noramlize our image (convert color channel values from 0-255 to 0-1)
6. Resize the `image` to be a shape of (224, 224)
5. Return the modified `image`

### Before we do, lets see what importing an image looks like
"""

# Convert image to NumPy array
from matplotlib.pyplot import imread
image = imread(filenames[42])
image.shape

image.max(), image.min()

#RGB array  Turn image into a Tensor
tf.constant(image)[:2]

"""Now we've seen what an image looks like as a tensor let's build a function to turn images into tensors"""

# Define image size
IMG_SIZE = 224

# Create a function for preprocessing images
def process_image(image_path, image_size=IMG_SIZE):
  """
  Takes an image file path and turn the image into a Tensor.
  """

  # Read in an image file
  image = tf.io.read_file(image_path)
  # Turn the jpeg image into numerical Tensor with 3 colour channels (Read, Green, Blue )
  image = tf.image.decode_jpeg(image, channels=3)
  # Convert the color channel values from 0-255 to 0-1 values
  image = tf.image.convert_image_dtype(image, tf.float32)
  #Resize the image to our desired value (244, 244)
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])

  return image

"""## Turning our data into batches

Why turn our data into batches?

Let's say you're trying to process 10,000+ images in one go... they all
might not fit into memory.

So that's why we do about 32 images (batch size) at at time. You can adjust the
batch size if need be.

In order to use TensoFlow effectively, we need our data in the form of Tensor tuples which look like this:

`(image, label)`
"""

# Create a simple function to return a tuple (image, label)
def get_image_label(image_path, label):
  """
  Takes an image file path name and the associated label,
  processes the image and returns a tuple of (image, label)
  """
  image = process_image(image_path)
  return image, label

# Demo of the above
(process_image(X[42]), tf.constant(y[42]))

"""Now we've got a way to turn our data into tuples of Tensors
in the form: `(image, label)` 

Let's make a function to turn all of our data (X & y) into batches!
"""

# Define the batch size, 32 is a good start
BATCH_SIZE = 32

# Create a function to turn data into batches
def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):
  """
  Creates batches of data out of image (X) and label (y) pairs.
  It shuffles the data if it's training data but doesn't shuffle if it's validation data.
  Also accepts test data as input (no labels).
  """
  # If the data is a test dataset, we probably don't have labels
  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths (no labels)
    data_batch = data.map(process_image).batch(BATCH_SIZE)
    return data_batch

  # If the data is a valid dataset, we don't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),  #filepaths
                                               tf.constant(y))) # labels
    data_batch = data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print("Creating training data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),  #filepaths
                                               tf.constant(y))) # labels
    # Suffling pathnames and labels before mapping image processor function if faster than shuffling images
    data = data.shuffle(buffer_size=len(X))
    # Creat (image, label) tuples (this also turns the image path into a preprocessed image)
    data = data.map(get_image_label)
    # Turn the train data into batches
    data_batch = data.batch(BATCH_SIZE)

    return data_batch

# Create training and validation data batches
train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val, y_val, valid_data=True)

# Check out the different attributes of our data batches
train_data.element_spec, val_data.element_spec

y[0]

"""## Visualizing Data Batches

Our data now is in batches, however, these can be a little hard to understand
,lets visualize the image

"""

import matplotlib.pyplot as plt

# Create a function for viewing images in a data batch
def show_25_images(images, labels):
  """
  Displays a plot of 25 images and their labels from a data batch
  """
  # Setup the figure
  plt.figure(figsize=(10, 10))
  # Loop through 25 (for displaying 25 images )
  for i in range(25):
    # create subplots (5 rows, 5 columns)
    ax = plt.subplot(5, 5, i+1)
    # Display an image
    plt.imshow(images[i])
    # Add the image label as the title
    plt.title(unique_breeds[labels[i].argmax()])
    # Turn the grid lines off
    plt.axis("off")

unique_breeds[y[0].argmax()]

train_data

# Now let's visualize the data in a training batch
train_images, train_labels = next(train_data.as_numpy_iterator())
show_25_images(train_images, train_labels)

len(train_images), len(train_labels)

# Now let's visualize out validation set
val_images, val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images, val_labels)

"""## Building a model

Before we build a model, there are a few things we need to define:
* The input shape (our images shape, in the form of Tensors) to our model.
* The output shape (image labels, in the form of Tensors) of our model.
* The URL of the model we want to use.
"""

# Setup input shape to the model
INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3]

# Setup output shape of our model
OUTPUT_SHAPE = len(unique_breeds)

# Setup model URL from TensorFlow Hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5"

"""Now we've got our inputs, outputs and model ready to go.
Let's put them together into a Keras deep learing model!

Knowing this, let's create a function which:
* Takes the input shape, output shape and the model we've
chosen as parameters.
* Defines the layers in a Keras model in sequential fashion
(do this first, then this, then that).
* Complies the model (says it should be valuated and improved).
* Builds the model (tells the model the input shape it'll be getting).
* Returns the model.

All of these steps can be found here
https://www.tensorflow.org/guide/keras/overview

"""

# create a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", MODEL_URL)

  # Setup the model layers
  model = tf.keras.Sequential([
    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)
    tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                          activation="softmax") # Layer 2 (output layers)
  ])

  # Compile the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"]
  )

  # Build the model
  model.build(INPUT_SHAPE)

  return model

model = create_model()
model.summary()

outputs = np.ones(shape=(1,1,1280))
outputs

"""## Creating callbacks

Callbacks are helper functions a model can use during training to do such things as save its progress, check its progress or stop training ealry if a model stops improving

We'll create two callbacks, one for TensorBoard which helps track our models progress and another for early stopping which prevents our model from training for too long.

### TensorBoard Callback:

1. Load the TensorBoard notebook extension ‚úÖ
2. Create a TensorBoard callback which is able to save logs to a directory and pass it to our model's fit() function.
3. Visualize our models training logs with the %tensorboard magic function (we'll do this after model training).
"""

# Commented out IPython magic to ensure Python compatibility.
# Load TensorBoard notebook extension
# %load_ext tensorboard

import datetime

# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log directory for storing Tensorboard logs
  logdir = os.path.join("drive/MyDrive/Dog Vision/logs",
                        # Make it so the logs get tracked whenever we run an experiment
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

"""## Early Stopping Callback

Early stopping helps stop our model from overfitting by stopping training if a certain evaluation metric stops

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/earlyStopping

"""

# Create early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                  patience=3)



"""## Training a model (on subset of data)

Our first model is only going to train on 1000 images, to make sure everything is working.
"""

NUM_EPOCHS = 100 #@param {type:"slider", min:10, max:100, step:10}

# Check to make sure we're still running on a GPU
print("GPU", "available (YESSS!!!)" if tf.config.list_logical_devices("GPU") else "not available")

"""###Let's create a function which trains a model.

* Create a model using `create_model()`
* Setup a TensorBoard callback using create_tensorboard_callback()
* Call the fit() function on our model passing it the training data, validation data, number of epochs to train for (`NUM_EPOCHS`) and the callbacks we'd like to use
* Return the model

"""

# Build a function to train and return a trained model
def train_model():
  """
  Trains a given model and returns the trained version.
  """
  # Create a model
  model = create_model()

  # Create new TensorBoard session everytime we train a model
  tensorboard = create_tensorboard_callback()

  # Fit the model to the training data passing it the callbacks we created
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard, early_stopping])
  # Return teh fitted model
  return model

# fit the model to the data
model = train_model()



"""**Question:** It looks like our model is overfitting because it's performing far better on the training dataset than the validation dataset, what are some ways to prevent model overfitting in deep learning neural networks?

**Note:** Overfitting to beginn with is a good thing! It means our model is learning!!

### Checking the TensorBoard logs

The TensorBoard magic function (%tensorboard) will access the logs directory we created earlier and visualize its contents
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/Dog\ Vision/logs

"""## Making and evaluating predictions using a trained model"""

val_data

# Make predictions on the validation data (not used to train on)
predictions = model.predict(val_data, verbose=1)
predictions

predictions.shape

len(y_val)

len(unique_breeds)

predictions[0]

len(predictions[0])

np.sum(predictions[0])

# First prediction
def predictions_index(index): 
  print(predictions[0])
  print(f"Max value (probability of prediction): {np.max(predictions[index])}")
  print(f"Sum: {np.sum(predictions[index])}")
  print(f"Max index: {np.argmax(predictions[index])}")
  print(f"Predicted label: {unique_breeds[np.argmax(predictions[index])]}")

predictions_index(0)

unique_breeds[119]

"""Having the above functionality is great but we want to be able to do it at scale.

And it would be even better if we could see the image the prediction is being made on!

**Note** Prediction probabilities are also known as confidence levels

"""

# Turn prediction probabilities into their respective label (easier to unserstand)
def get_pred_label(prediction_probabilities):
  """
  Turns an array of predictions into a label.
  """
  return unique_breeds[np.argmax(prediction_probabilities)]


# Get a predicted label based on an array of prediction probabilities
pred_label = get_pred_label(predictions[81])
pred_label

val_data

"""Now since our validation data is still in a batch dataset,
We'll have to unbatchify it to make predictions on the validation images and then compare those predictions to the validation labels (truth label)
"""

images_ = []
labels_ = []

# Loop through unbatched data
for image, label in val_data.unbatch().as_numpy_iterator():
  images_.append(image)
  labels_.append(label)

images_[0], labels_[0]

get_pred_label(labels_[0])

# Create a function to unbath a batch dataset
def unbatchify(data):
  """
  Takes a batched dataset of (image, label) Tensors and return separate arrays of images and labels
  """
  images = []
  labels = []
  # Loog through unbatched data
  for image, label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])

  return images, labels

#Unbatchify the validationdata
val_images, val_labels = unbatchify(val_data)
val_images[0], val_labels[0]

get_pred_label(val_labels[0])

"""Now we've got wyas to get:
* Prediction labels
* Validation labels (truth labels)
* Validation images

Let's make some function to make these all a bit more visualize.

We'll create a function which:
* Takes an array of prediction probabilities, an array of truth labels and an array of images and integers.
* Convert the prediction probabilities to a predicted label.
* Plot the predicted label, it's predicted probability, the truth label and the target image on a single plot.
"""

def plot_pred(prediction_probabilities, labels, images, n=1):
  """
  View the prediction, ground truth and image for sample_n
  """
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

  # Get the pred label
  pred_label = get_pred_label(pred_prob)

  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the color of the title depnding of if the predition is right or wrong
  if pred_label == true_label:
    color = "green"
  else:
    color = "red"


  # Change plot tile to be predicted, probaility of prediction and truth label
  plt.title("{} {:2.0f}% {}".format(pred_label,
                                    np.max(pred_prob)*100,
                                    true_label),
                                    color=color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,
          n=77)

"""Now we've got one function to visualize our models top prediction, let's make another to view our models top 10 predictions

This function will:
*  Take an input of prediction probabilities array and a ground truth array and an integer
* Find the prediction using `get_pred_label()`
* Find the top 10:
  * Prediction probabilities indexes
  * Prediction probabilities values
  * Prediction labels
* Plot the top 10 prediction probability values and labels, coloring the true label green

"""

def plot_pred_conf(prediction_probabilities, labels, n=1):
  """
  Plus the top 10 highest prediction confidences along with the truth label for sample n.
  """
  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]

  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]

  # Find the top 10 prediction labels
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values,
                     color="grey")
  
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")
  
  # Change color of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=9)

predictions[0][predictions[0].argsort()[-10:][::-1]]

predictions[0].max()

unique_breeds[predictions[0].argsort()[-10:][::-1]]

"""Now we've got some function to help us visualize our predictions and evaluate our model. Let's check out a few predictions."""

# Let's check out a few predictions and their different values
def plot_predictions(i_multiplier=10, num_rows=3, num_cols=2):
  """
  Show a graph of a few predictions
  """
  num_images = num_rows*num_cols
  plt.figure(figsize=(10*num_cols, 5*num_rows))
  for i in range(num_images):
    plt.subplot(num_rows, 2*num_cols, 2*i+1)
    plot_pred(prediction_probabilities=predictions,
              labels=val_labels,
              images=val_images,
              n=i+i_multiplier)
    plt.subplot(num_rows, 2*num_cols, 2*i+2)
    plot_pred_conf(prediction_probabilities=predictions,
                  labels=val_labels,
                  n=i+i_multiplier)
  plt.tight_layout(h_pad=1.0)
  plt.show()

plot_predictions()

"""**Challenge:** How would you creae a confusion matrix with our models predictions and true labels?

##Saving and Reloading our models
"""

# Create a function to save a model
def save_model(model, suffix=None):
  """
  Saves a given model in a models directory and appends a suffix (string).
  """
  # Create a model directory pathname with current time
  modeldir = os.path.join("drive/MyDrive/Dog Vision/models",
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5" #save format for model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

# Load a train model
def load_model(model_path):
  """
  Load a trained model from our specify path
  """
  print(f"Loading saved model from: {model_path}...")
  model = tf.keras.models.load_model(model_path,
                                     custom_objects={"KerasLayer":hub.KerasLayer})
  
  return model

"""Now we've got functions to load and save model"""

# Save our model trained on 1000 images
save_model(model, suffix="1000-images-mobilenetv2-Adam")

# Load our model
loaded_1000_image_model = load_model("drive/MyDrive/Dog Vision/models/20220323-15081648048130-1000-images-mobilenetv2-Adam.h5")

# Evaluate the pre-saved model
model.evaluate(val_data)

# Evaluate the loaded model
loaded_1000_image_model.evaluate(val_data)

"""## Training a big dog model on the full data üê∂"""

len(X), len(y)

# Create a data batch with the full dataset
full_data = create_data_batches(X, y)

full_data

# Create a model for full model
full_model = create_model()

# Create full model callbacks
full_model_tensorboard = create_tensorboard_callback()
# No validation set when training on all the data, so we can't monitor validaton accurarcy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy",
                                                             patience=3)

"""**Note:** Running the cell below will take a little while
(maybe up to 30 minutes for the first epoch) because the GPU we're using in the runtime has to load all the model of the images into memory.
"""

# Fit the full model to the full data
full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_tensorboard, full_model_early_stopping])

save_model(full_model, suffix="full_image1-set-mobilenetv2-Adam")

# Load in the full model
loaded_full_model = load_model("drive/MyDrive/Dog Vision/models/20220324-00041648080268-full_image1-set-mobilenetv2-Adam.h5")

len(X)

"""## Making prediction on the test dataset

Since our model has been trained on images in the form of Tensor batches, to make pre dictions on the test data, we'll have to get it into the same format.

Luckily we created `create_data_batches()` earlier which can take a list of filenames as input and conver them into Tensor batches.

To make predictions on the test data, we'll:
* Get the test image filenames
* Convert the filenames into test data batches using 
`create_data_batches` and setting the `test_data`
parameter to `True` (since the test data doesn't have labels).
* Make a predictions array by passing the test batches to the predict() method called on our model
"""

# Load test image filenames
test_path = "drive/MyDrive/Dog Vision/test/"
test_filenames = [test_path + fname for fname in os.listdir(test_path)]
test_filenames[:10]

len(test_filenames)

# Create test data batch
test_data = create_data_batches(test_filenames, test_data=True)

test_data

"""**Note** Calling `predict()` on our full model passing it our test data batch will take a long time"""

# Make predictions on test data batch using the loaded full model
test_predictions = loaded_full_model.predict(test_data, verbose=1)

# Save predictions (NumPy array) to csv file for access later
np.savetxt("drive/MyDrive/Dog Vision/preds_array1.csv", test_predictions, delimiter=",")

# Load predictions (NumPy array) from csv file
test_predictions_loaded = np.loadtxt("drive/MyDrive/Dog Vision/preds_array1.csv", delimiter=",")

test_predictions[:10]

test_predictions.shape

"""## Preparing test dataset prediction for Kaggle

Looking at the Kaggle sample submission, we find that it wants our models prediction probability outpus in a DataFrame (CSV) with an ID and a column for each different dog breed.
https://www.kaggle.com/c/dog-breed-identification/overview/evaluation

https://www.kaggle.com/c/dog-breed-identification/data?select=sample_submission.csv

To get the data in this format, we'll:
* Create a pandas DataFrame with an ID column as well as a column for each dog breed ‚úÖ
* Add data to the ID column by extracting the test image
ID's from their filepaths
* Add data (the prediction probabilities) to each of the dog breed columns.
* Export the DataFrame as a CSV to submit to Kaggle

"""

# Create a pandas DataFrame with empty columns
preds_df = pd.DataFrame(columns=["id"] + list(unique_breeds))
preds_df.head()

# Append test image ID's to predictions DataFrame
test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
preds_df["id"] = test_ids

preds_df.head()

# Add the prediction probabilities to each dog breed column
preds_df[list(unique_breeds)] = test_predictions
preds_df.head()

# Save our predictions dataframe to CSV for submission to Kaggel
preds_df.to_csv("drive/MyDrive/Dog Vision/full_model_predictions_submission_2_mobilenetV2.csv", index=False)

len(preds_df)

"""### Making prediction on custom images

To make predictions on custom images, we'll:
* Get the filepaths of our own images.
* Turn the filepaths into data batches using create_data_batches(). And since our custom images won't have labels, we set the test_data parameter to True.
* Pass the custom images data batch to our models predict() methond.
* Convert the prediction output probabilities to predictions labers.
* Compare the predicted labels to the cutom images

"""

# Get the custom image filepaths
custom_path = "drive/MyDrive/Dog Vision/DogsPhoto/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path)]

custom_image_paths

# Turn custom images into batch datasets
custom_data = create_data_batches(custom_image_paths, test_data=True)
custom_data

# Make predictions on the custom data
custom_preds = loaded_full_model.predict(custom_data)

custom_preds.shape

# Get custom image prediction labels
custom_preds_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_preds_labels

# Get custom images (out unbatchcify() won't work because no labels)

custom_images = []

# Loop through unbatched data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

# Check custom image predictions
plt.figure(figsize=(10,10))
for i, image in enumerate(custom_images):
  #plt.subplot(1, 11, i+1)
  #plt.xticks([])
  #plt.yticks([])
  #plt.title(custom_preds_labels[i])  
  #plt.imshow(image)

  # create subplots (5 rows, 5 columns)
  ax = plt.subplot(5, 5, i+1)
  # Display an image
  plt.imshow(image)
  # Add the image label as the title
  plt.title(custom_preds_labels[i])
  # Turn the grid lines off
  plt.axis("off")



